{
 "metadata": {
  "name": "",
  "signature": "sha256:7da5b792e1866c150f59b172f28a14ec5e92e4a25ef943c9dfddc6de2bcc0777"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from ale_python_interface import ALEInterface\n",
      "import tensorflow as tf\n",
      "import numpy as np\n",
      "import cv2\n",
      "import random\n",
      "import threading\n",
      "\n",
      "import sys\n",
      "import time\n",
      "import os\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "%matplotlib inline\n",
      "\n",
      "from replayMemory import ReplayMemory\n",
      "from buildGraph import createQNetwork, build_train_op"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "/home/cgel/.local/lib/python2.7/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
        "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ale = ALEInterface()\n",
      "viz = False\n",
      "rom_name = \"roms/Breakout.bin\"\n",
      "ale.setBool('sound', False)                                                                                \n",
      "ale.setBool('display_screen', viz) \n",
      "ale.setInt(\"frame_skip\", 4)\n",
      "ale.loadROM(rom_name)\n",
      "legal_actions = ale.getMinimalActionSet()\n",
      "action_map = {}\n",
      "for i in range(len(legal_actions)):\n",
      "    action_map[i] = legal_actions[i]\n",
      "action_num = len(action_map)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class config:\n",
      "    batch_size = 32\n",
      "    action_num = action_num\n",
      "    replay_memory_capacity = 1000000\n",
      "    steps_before_training = 12500\n",
      "    buff_size = 4\n",
      "    device = \"/gpu:0\"\n",
      "    gamma = 0.99\n",
      "    learning_rate = 0.00025\n",
      "    exploration_steps = 250000\n",
      "    initial_epsilon = 1.\n",
      "    final_epsilon = 0.1\n",
      "    sync_rate = 2500\n",
      "    save_summary_rate = 5000   \n",
      "    \n",
      "def get_epsilon():\n",
      "    if global_step < config.exploration_steps:\n",
      "        return config.initial_epsilon-((config.initial_epsilon-config.final_epsilon)/config.exploration_steps)*global_step\n",
      "    else:\n",
      "        return config.final_epsilon\n",
      "    \n",
      "RM = ReplayMemory(config)\n",
      "\n",
      "def flush_print(str):\n",
      "    print(str)\n",
      "    sys.stdout.flush()\n",
      "    \n",
      "def preprocess(new_frame, state):\n",
      "    frame = cv2.resize(new_frame, (84, 84))\n",
      "    new_state = np.roll(state, -1, axis=3)\n",
      "    new_state[0, :, :, config.buff_size -1] = frame\n",
      "    return new_state"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "with tf.device(config.device):\n",
      "    input_state_ph = tf.placeholder(tf.float32,[config.batch_size,84,84,4], name=\"input_state_ph\")\n",
      "    # this should be: input_state_placeholder = tf.placeholder(\"float\",[None,84,84,4], name=\"state_placeholder\")\n",
      "    action_ph = tf.placeholder(tf.int64, [config.batch_size], name=\"Action_ph\")\n",
      "    Y_ph = tf.placeholder(tf.float32, [config.batch_size], name=\"Y_ph\")\n",
      "\n",
      "    ph_lst = [input_state_ph, action_ph, Y_ph]\n",
      "\n",
      "    q = tf.FIFOQueue(2, [ph.dtype for ph in ph_lst],\n",
      "                     [ph.get_shape() for ph in ph_lst])\n",
      "    enqueue_op = q.enqueue(ph_lst)\n",
      "    input_state, action, Y = q.dequeue()\n",
      "\n",
      "    # so that i can feed inputs with different batch sizes.\n",
      "    input_state = tf.placeholder_with_default(input_state, shape=tf.TensorShape([None]).concatenate(input_state.get_shape()[1:]))\n",
      "    action = tf.placeholder_with_default(action, shape=[None])\n",
      "    next_input_state_ph = tf.placeholder(tf.float32,[config.batch_size,84,84,4], name=\"next_input_state_placeholder\")\n",
      "\n",
      "    with tf.variable_scope(\"DQN\"):\n",
      "        Q = createQNetwork(input_state, action, config, \"DQN\")\n",
      "        DQN_params = tf.get_collection(\"DQN_weights\")\n",
      "        max_action_DQN = tf.argmax(Q, 1)\n",
      "    with tf.variable_scope(\"DQNTarget\"):\n",
      "        # pasing an action is useless because the target never runs the next_Y_prediction but it is needed for the code to work\n",
      "        QT = createQNetwork(next_input_state_ph, action, config, \"DQNT\")\n",
      "        DQNT_params = tf.get_collection(\"DQNT_weights\")\n",
      "        DQNT_params = tf.get_collection(\"DQNT_weights\")\n",
      "        max_QT = tf.reduce_max(QT, 1)\n",
      "        tf.add_to_collection(\"DQNT_summaries\", tf.scalar_summary(\n",
      "            \"main/next_Q_max\", tf.reduce_max(QT)))\n",
      "        tf.add_to_collection(\"DQNT_summaries\", tf.scalar_summary(\n",
      "            \"main/next_Q_0\", max_QT[0]))\n",
      "\n",
      "    # DQN summary\n",
      "    for i in range(action_num):\n",
      "        dqni = tf.scalar_summary(\"DQN/action\"+str(i), Q[0, i])\n",
      "        tf.add_to_collection(\"DQN_summaries\", dqni)\n",
      "\n",
      "    sync_DQNT_op = [DQNT_params[i].assign(DQN_params[i]) for i in range(len(DQN_params))]\n",
      "\n",
      "    train_op = build_train_op(Q, Y, action, config)\n",
      "\n",
      "\n",
      "\n",
      "def enqueue_from_RM():\n",
      "    while True:\n",
      "        state_batch, action_batch, reward_batch, next_state_batch, terminal_batch, _ = RM.sample_transition_batch()\n",
      "        if global_step % config.save_summary_rate == 0:\n",
      "            QT_np, DQNT_summary_str = sess.run([QT, DQNT_summary_op], feed_dict={next_input_state_ph:next_state_batch})\n",
      "            summary_writter.add_summary(tf.Summary(value=[tf.Summary.Value(tag=\"main/r_max\", simple_value=int(np.max(reward_batch)))]), global_step)\n",
      "            summary_writter.add_summary(DQNT_summary_str, global_step)\n",
      "        else:\n",
      "            QT_np = sess.run(QT, feed_dict={next_input_state_ph:next_state_batch})\n",
      "\n",
      "        DQNT_max_action_batch = np.max(QT_np, 1)\n",
      "\n",
      "        Y = []\n",
      "        for i in range(state_batch.shape[0]):\n",
      "            terminal = terminal_batch[i]\n",
      "            if terminal:\n",
      "                Y.append(reward_batch[i])\n",
      "            else:\n",
      "                Y.append(reward_batch[i] + config.gamma * DQNT_max_action_batch[i])\n",
      "        feed_dict={input_state_ph:state_batch, action_ph:action_batch, Y_ph:Y}\n",
      "        sess.run(enqueue_op, feed_dict=feed_dict)\n",
      "\n",
      "enqueue_from_RM_thread = threading.Thread(target=enqueue_from_RM)\n",
      "enqueue_from_RM_thread.daemon = True\n",
      "\n",
      "\n",
      "timeout_option = tf.RunOptions(timeout_in_ms=5000)\n",
      "def update_params():\n",
      "    if global_step > config.steps_before_training:\n",
      "        if enqueue_from_RM_thread.isAlive() == False:\n",
      "            flush_print(\"starting enqueue thread\")\n",
      "            enqueue_from_RM_thread.start()\n",
      "\n",
      "        if global_step % config.save_summary_rate == 0:\n",
      "            _, DQN_summary_str = sess.run([train_op, DQN_summary_op], options=timeout_option)\n",
      "            summary_writter.add_summary(DQN_summary_str, global_step)\n",
      "        else:\n",
      "             _ = sess.run(train_op, options=timeout_option)\n",
      "\n",
      "        if global_step % config.sync_rate == 0:\n",
      "            sess.run(sync_DQNT_op)\n",
      "\n",
      "\n",
      "sess_config = tf.ConfigProto()\n",
      "sess_config.allow_soft_placement = True\n",
      "sess_config.gpu_options.allow_growth = True\n",
      "sess_config.log_device_placement = False\n",
      "sess = tf.Session(config=sess_config)\n",
      "saver = tf.train.Saver(DQN_params, max_to_keep = 20)\n",
      "sess.run(tf.initialize_variables(DQN_params))\n",
      "sess.run(tf.initialize_variables(DQNT_params))\n",
      "sess.run(tf.initialize_all_variables())\n",
      "\n",
      "\n",
      "#geneate a new set of paths\n",
      "run_list = os.listdir(\"log\")\n",
      "int_run_list = [int(r) for r in run_list] + [0]\n",
      "run_name = str(max(int_run_list) + 1)\n",
      "#run_name = str(3)\n",
      "checkpoint_path = \"checkpoint/\" + run_name + \".ckpt\"\n",
      "log_path = \"log/\"+ run_name\n",
      "print(run_name)\n",
      "DQN_summary_op = tf.merge_summary(tf.get_collection(\"DQN_summaries\") + \\\n",
      "                                  tf.get_collection(\"DQN_prediction_summaries\"))\n",
      "DQNT_summary_op = tf.merge_summary(tf.get_collection(\"DQNT_summaries\"))\n",
      "summary_writter = tf.train.SummaryWriter(log_path, sess.graph, flush_secs=20)\n",
      "\n",
      "\n",
      "def e_greedy_action(epsilon, state):\n",
      "    if np.random.uniform() < epsilon:\n",
      "        action = random.randint(0, action_num - 1)\n",
      "    else:\n",
      "        QS = sess.run(Q, feed_dict={input_state:state})[0]\n",
      "        action = np.argmax(QS)\n",
      "    return action\n",
      "\n",
      "def greedy_run(epsilon, n, use_planning=False):\n",
      "    ale.reset_game()\n",
      "    R_list = []\n",
      "    for episode in range(n):\n",
      "        state = np.zeros((1, 84, 84, config.buff_size), dtype=np.uint8)\n",
      "        state = preprocess(ale.getScreenGrayscale(), state)\n",
      "        R = 0\n",
      "        while ale.game_over() == False:\n",
      "            if use_planning:\n",
      "                action = e_greedy_planning_action(epsilon, state)\n",
      "            else:\n",
      "                action = e_greedy_action(epsilon, state)\n",
      "            reward = ale.act(action_map[action])\n",
      "            state = preprocess(ale.getScreenGrayscale(), state)\n",
      "            R += reward\n",
      "        R_list.append(R)\n",
      "        ale.reset_game()\n",
      "    return R_list\n",
      "\n",
      "global_step = 0\n",
      "global_episode = 0\n",
      "logging = True"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "105\n",
        "=="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.263037800789\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "t = time.time()\n",
      "num_episodes = 100\n",
      "initial_episode = global_episode\n",
      "sess.run(sync_DQNT_op)\n",
      "for episode in range(global_episode, num_episodes + global_episode):\n",
      "    global state\n",
      "    state = np.zeros((1, 84, 84, config.buff_size), dtype=np.uint8)\n",
      "    state = preprocess(ale.getScreenGrayscale(), state)\n",
      "    R = 0\n",
      "    ep_begin_t = time.time()\n",
      "    terminal = False\n",
      "    pseudo_terminal = False\n",
      "    lives = ale.lives()\n",
      "    episode_begining_step = global_step\n",
      "    while terminal == False:\n",
      "        action = e_greedy_action(get_epsilon(), state)\n",
      "        reward = ale.act(action_map[action])\n",
      "        clipped_reward = max(-1, min(1, reward))\n",
      "        R += reward\n",
      "        pseudo_terminal = False\n",
      "        if ale.game_over():\n",
      "            terminal = True\n",
      "        if lives != ale.lives() or terminal:\n",
      "            lives = ale.lives()\n",
      "            pseudo_terminal = True\n",
      "        RM.add(state[0, :, :, config.buff_size -1], action, clipped_reward, pseudo_terminal)\n",
      "        update_params()\n",
      "        state = preprocess(ale.getScreenGrayscale(), state)\n",
      "        global_step += 1\n",
      "    ep_duration = time.time() - ep_begin_t\n",
      "    if logging and episode%100 == 0 and episode != 0 or num_episodes == episode:\n",
      "        episode_online_summary = tf.Summary(value=[tf.Summary.Value(tag=\"online/epsilon\", simple_value=get_epsilon()),\n",
      "                                    tf.Summary.Value(tag=\"online/R\", simple_value=R),\n",
      "                                    tf.Summary.Value(tag=\"online/steps_in_episode\", simple_value= global_step - episode_begining_step),\n",
      "                                    tf.Summary.Value(tag=\"online/global_step\", simple_value = global_step),\n",
      "                                    tf.Summary.Value(tag=\"online/ep_duration_seconds\", simple_value=ep_duration)])\n",
      "        summary_writter.add_summary(episode_online_summary, global_episode)\n",
      "    # log percent\n",
      "    if logging and logging==True and episode%500 == 0 and episode != 0 or num_episodes == episode:\n",
      "        percent = int(float(episode - initial_episode)/num_episodes * 100)\n",
      "        print(\"%i%% -- epsilon:%.2f\"%(percent, get_epsilon()))\n",
      "    # save\n",
      "    if logging and episode%1000 == 0 and episode != 0 or num_episodes == episode:\n",
      "        print(\"saving checkpoint at episode \" + str(episode))\n",
      "        saver.save(sess, checkpoint_path, episode)\n",
      "\n",
      "    # performance summary\n",
      "    if logging and episode%1000 == 0 and episode != 0 or num_episodes == episode:\n",
      "        R_list = greedy_run(epsilon = 0.05, n=20)\n",
      "        performance_summary = tf.Summary(value=[tf.Summary.Value(tag=\"R/average\", simple_value=sum(R_list)/len(R_list)),\n",
      "                                      tf.Summary.Value(tag=\"R/max\", simple_value=max(R_list)),\n",
      "                                      tf.Summary.Value(tag=\"R/min\", simple_value=min(R_list))])\n",
      "        summary_writter.add_summary(performance_summary, global_step)\n",
      "\n",
      "    global_episode += 1\n",
      "    ale.reset_game()\n",
      "print(\"==\")\n",
      "print((time.time() - t)/60)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0% -- epsilon:0.98\n",
        "saving checkpoint at episode 100\n",
        "=="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "0.638917966684\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "c = 0\n",
      "t = 0\n",
      "for i in range(1000):\n",
      "    c = c+32\n",
      "    state_batch, action_batch, reward_batch, next_state_batch, terminal_batch, _ = RM.sample_transition_batch()\n",
      "    t = t+ terminal_batch.sum()\n",
      "\n",
      "print(float(c)/t)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "33.2986472425\n"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}